{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Hyperparameter tuning on XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "Example from https://medium.com/analytics-vidhya/hyperparameter-tuning-hyperopt-bayesian-optimization-for-xgboost-and-neural-network-8aedf278a1c9"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Step 0: Load required packages and create a toy-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features size:   600 x 25\n",
      "Testing features size:    200 x 25\n",
      "Validation features size: 200 x 25\n",
      "\n",
      "Y_train\n",
      "Class: 0, Count: 300, Percentage: 50.0%\n",
      "Class: 1, Count: 300, Percentage: 50.0%\n",
      "\n",
      "Y_test\n",
      "Class: 0, Count: 103, Percentage: 51.5%\n",
      "Class: 1, Count:  97, Percentage: 48.5%\n",
      "\n",
      "Y_validation\n",
      "Class: 1, Count: 102, Percentage: 51.0%\n",
      "Class: 0, Count:  98, Percentage: 49.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "seed = 42 # Set seed for reproducibility purposes\n",
    "metric = 'accuracy' # See other options https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "kFoldSplits = 5\n",
    "\n",
    "np.random.seed(seed) # Set numpy seed for reproducibility\n",
    "\n",
    "# Create a toy-dataset using make_classification function from scikit-learn\n",
    "X,Y=make_classification(n_samples=1000,\n",
    "                        n_features=25,\n",
    "                        n_informative=2,\n",
    "                        n_redundant=10,\n",
    "                        n_classes=2,\n",
    "                        random_state=seed)\n",
    "\n",
    "# Split in train-test-validation datasets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=0.25, random_state=seed) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Check on created data\n",
    "print(\"Training features size:   %s x %s\\nTesting features size:    %s x %s\\nValidation features size: %s x %s\\n\" % (X_train.shape[0],X_train.shape[1], \n",
    "                                                                                                                     X_test.shape[0],X_test.shape[1], \n",
    "                                                                                                                     X_validation.shape[0],X_validation.shape[1]))\n",
    "\n",
    "# Create a function to print variable name\n",
    "def namestr(obj, namespace = globals()):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "# Check on class distribution\n",
    "for x in [Y_train, Y_test, Y_validation]:\n",
    "    print(namestr(x)[0])\n",
    "    counter = Counter(x)\n",
    "    for k,v in counter.items():\n",
    "        pct = v / len(x) * 100\n",
    "        print(\"Class: %1.0f, Count: %3.0f, Percentage: %.1f%%\" % (k,v,pct))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "### Step 1: Initialize space or a required range of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "       'gamma': hp.uniform ('gamma', 1, 9),\n",
    "       'reg_alpha' : hp.quniform('reg_alpha', 40, 180, 1),\n",
    "       'reg_lambda' : hp.uniform('reg_lambda', 0, 1),\n",
    "       'colsample_bytree' : hp.uniform('colsample_bytree', 0.5, 1),\n",
    "       'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "       'n_estimators': hp.quniform('n_estimators', 50, 250, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "**Other available hyperopt optimization algorithms are can be found [here](https://github.com/hyperopt/hyperopt/wiki/FMin).**\n",
    "* hp.choice(label, options) — Returns one of the options, which should be a list or tuple.<br/>\n",
    "* hp.randint(label, upper) — Returns a random integer between the range (0, upper), 0 included.<br/>\n",
    "* hp.uniform(label, low, high) — Returns a value uniformly between low and high.<br/>\n",
    "* hp.quniform(label, low, high, q) — Returns a value round(uniform(low, high) / q) * q, i.e it rounds the decimal values and returns an integer.<br/>\n",
    "* hp.normal(label, mean, std) — Returns a real value that’s normally-distributed with mean and standard deviation sigma."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Step 2: Define objective function\n",
    "### Standard approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If regression, then: \n",
    "def hyperparameter_tuning(space):\n",
    "    global best_score\n",
    "    \n",
    "    reg=xgb.XGBRegressor(n_estimators = int(space['n_estimators']), max_depth = int(space['max_depth']), gamma = space['gamma'],\n",
    "                         reg_alpha = int(space['reg_alpha']),min_child_weight=space['min_child_weight'],\n",
    "                         colsample_bytree=space['colsample_bytree'])\n",
    "    \n",
    "    evaluation = [(X_train, Y_train), (X_test, Y_test)]\n",
    "    \n",
    "    reg.fit(X_train, y_train,\n",
    "            eval_set = evaluation, eval_metric = \"rmse\",\n",
    "            early_stopping_rounds = 10,verbose = False)\n",
    "\n",
    "    pred = reg.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, pred)\n",
    "    \n",
    "    if (mse < best_score):\n",
    "        best_score=mse\n",
    "        \n",
    "    # Change the metric according to the needs\n",
    "    return {'loss':mse, 'status': STATUS_OK}\n",
    "    \n",
    "# If classifier (our case), then:\n",
    "def hyperparameter_tuning(space):\n",
    "    global best_score\n",
    "    \n",
    "    clf = XGBClassifier(n_estimators = int(space['n_estimators']), max_depth = int(space['max_depth']), gamma = space['gamma'],\n",
    "                        reg_alpha = int(space['reg_alpha']),min_child_weight=space['min_child_weight'],\n",
    "                        colsample_bytree=space['colsample_bytree'])\n",
    "    \n",
    "    evaluation = [(X_train, Y_train), (X_test, Y_test)]\n",
    "    \n",
    "    clf.fit(X_train, Y_train,\n",
    "            eval_set = evaluation, eval_metric = 'logloss',\n",
    "            early_stopping_rounds = 10, verbose = False)\n",
    "\n",
    "    pred = clf.predict(X_test)\n",
    "    accuracy = 1-accuracy_score(Y_test, pred>0.5)\n",
    "    \n",
    "    if (accuracy < best_score):\n",
    "        best_score = accuracy\n",
    "    \n",
    "    # Change the metric according to the needs\n",
    "    return {'loss': accuracy, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Step 3: Run Hyperopt function\n",
    "### Standard approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.54trial/s, best loss: 0.11499999999999999]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "start = time.time()\n",
    "neval = 100\n",
    "best_score = 1.0\n",
    "\n",
    "best = fmin(fn = hyperparameter_tuning,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = neval,\n",
    "            trials = trials,\n",
    "            rstate = np.random.RandomState(seed))\n",
    "\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters optimization took 8 seconds for 100 candidates. Accuracy reached: 0.885\n",
      "Optimal parameters found:\n",
      "{'colsample_bytree': 0.5422824457728025, 'gamma': 1.2023173676497805, 'max_depth': 16.0, 'min_child_weight': 3.0, 'n_estimators': 177.0, 'reg_alpha': 56.0, 'reg_lambda': 0.9125226781143123}\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameters optimization took %.0f seconds for %d candidates. Accuracy reached: %.3f\\nOptimal parameters found:\\n%s\" % (elapsed_time, neval, (1-best_score), best))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Step 2: Define objective function\n",
    "### k-Fold Cross Validation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={'max_depth':hp.quniform('max_depth', 1, 50, 1),\n",
    "       'eta':hp.uniform ('eta', 0, 0.5),\n",
    "       'subsample':hp.uniform ('subsample', 0, 1),\n",
    "       'colsample_bylevel':hp.uniform ('colsample_bylevel', 0, 1),\n",
    "       'colsample_bytree':hp.uniform ('colsample_bytree', 0, 1),\n",
    "       'n_estimators':hp.quniform('n_estimators', 25, 500, 5),\n",
    "       'gamma': hp.uniform ('gamma', 1, 25),\n",
    "       'reg_alpha' : hp.quniform('reg_alpha', 25, 500, 1),\n",
    "       'reg_lambda' : hp.uniform('reg_lambda', 0, 1),\n",
    "       'min_child_weight' : hp.quniform('min_child_weight', 0, 50, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering only our classification tuning:\n",
    "def xgboost_tuning(space, kFoldSplits = 5, seed = 42, metric = 'accuracy'):\n",
    "    \n",
    "    global best_score, score_history, best_score_history\n",
    "    \n",
    "    clf = XGBClassifier(eta = space['eta'],\n",
    "                        subsample = space['subsample'],\n",
    "                        n_estimators = int(space['n_estimators']), \n",
    "                        max_depth = int(space['max_depth']), \n",
    "                        gamma = space['gamma'],\n",
    "                        reg_alpha = int(space['reg_alpha']),\n",
    "                        reg_lambda = space['reg_lambda'],\n",
    "                        min_child_weight=space['min_child_weight'],\n",
    "                        colsample_bytree=space['colsample_bytree'],\n",
    "                        colsample_bylevel=space['colsample_bylevel'], n_jobs=-1)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=kFoldSplits, random_state=seed, shuffle=True)\n",
    "    accuracy = 1-cross_val_score(clf, X_train, Y_train, cv=kfold, scoring=metric, verbose=False).mean() \n",
    "    \n",
    "    if (accuracy < best_score):\n",
    "        best_score = accuracy\n",
    "    \n",
    "    best_score_history.append(1-best_score)\n",
    "    score_history.append(1-accuracy)\n",
    "    \n",
    "    # Change the metric according to the needs\n",
    "    return {'loss': accuracy, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Step 3: Run Hyperopt function\n",
    "### k-Fold Cross Validation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:50<00:00,  2.17trial/s, best loss: 0.135]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "start = time.time()\n",
    "neval = 500\n",
    "best_score = 1\n",
    "score_history = []\n",
    "best_score_history = []\n",
    "\n",
    "best = fmin(fn = xgboost_tuning,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = neval,\n",
    "            trials = trials,\n",
    "            rstate = np.random.RandomState(seed))\n",
    "\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters optimization took 231 seconds for 500 candidates. Accuracy reached: 0.865\n",
      "\n",
      "Optimal parameters found:\n",
      "{'colsample_bylevel': 0.351270267642333, 'colsample_bytree': 0.9306756374972238, 'eta': 0.44678114613647835, 'gamma': 5.509039032472516, 'max_depth': 12.0, 'min_child_weight': 32.0, 'n_estimators': 55.0, 'reg_alpha': 50.0, 'reg_lambda': 0.2555624169299126, 'subsample': 0.9577836390233896}\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameters optimization took %.0f seconds for %d candidates. Accuracy reached: %.3f\\n\\nOptimal parameters found:\\n%s\" % (elapsed_time, neval, (1-best_score), best))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Step 4: Fit model using best parameters\n",
    "### k-Fold Cross Validation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(eta = best['eta'],\n",
    "                      subsample = best['subsample'],\n",
    "                      n_estimators = int(best['n_estimators']), \n",
    "                      max_depth = int(best['max_depth']), \n",
    "                      gamma = best['gamma'],\n",
    "                      reg_alpha = best['reg_alpha'],\n",
    "                      reg_lambda = best['reg_lambda'],\n",
    "                      min_child_weight=best['min_child_weight'],\n",
    "                      colsample_bytree=best['colsample_bytree'],\n",
    "                      colsample_bylevel=best['colsample_bylevel'],\n",
    "                      n_jobs=-1)\n",
    "\n",
    "evaluation = [(X_train, Y_train), (X_test, Y_test)]\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          eval_set = evaluation, eval_metric = 'logloss',\n",
    "          early_stopping_rounds = 10, verbose = False)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "accuracy = 1-accuracy_score(Y_test, pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy reached with best params on test set: 0.860\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy reached with best params on test set: %.3f\" % (1-accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
